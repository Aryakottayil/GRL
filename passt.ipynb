{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95138219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hear21passt in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (0.0.26)\n",
      "Requirement already satisfied: timm>=0.4.12 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from hear21passt) (1.0.15)\n",
      "Requirement already satisfied: torchaudio>=0.7.0 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from hear21passt) (2.5.1)\n",
      "Requirement already satisfied: torch in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from timm>=0.4.12->hear21passt) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from timm>=0.4.12->hear21passt) (0.20.1)\n",
      "Requirement already satisfied: pyyaml in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from timm>=0.4.12->hear21passt) (6.0.2)\n",
      "Requirement already satisfied: huggingface_hub in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from timm>=0.4.12->hear21passt) (0.32.4)\n",
      "Requirement already satisfied: safetensors in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from timm>=0.4.12->hear21passt) (0.5.3)\n",
      "Requirement already satisfied: filelock in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (2025.5.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (25.0)\n",
      "Requirement already satisfied: requests in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (4.14.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from huggingface_hub->timm>=0.4.12->hear21passt) (1.1.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from requests->huggingface_hub->timm>=0.4.12->hear21passt) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from requests->huggingface_hub->timm>=0.4.12->hear21passt) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from requests->huggingface_hub->timm>=0.4.12->hear21passt) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from requests->huggingface_hub->timm>=0.4.12->hear21passt) (2025.4.26)\n",
      "Requirement already satisfied: networkx in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from torch->timm>=0.4.12->hear21passt) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from torch->timm>=0.4.12->hear21passt) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from torch->timm>=0.4.12->hear21passt) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from sympy==1.13.1->torch->timm>=0.4.12->hear21passt) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from jinja2->torch->timm>=0.4.12->hear21passt) (3.0.2)\n",
      "Requirement already satisfied: numpy in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from torchvision->timm>=0.4.12->hear21passt) (2.2.6)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages (from torchvision->timm>=0.4.12->hear21passt) (11.2.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hear21passt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b01985fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "AugmentMelSTFT(\n",
      "  winsize=800, hopsize=320\n",
      "  (freqm): FrequencyMasking()\n",
      "  (timem): TimeMasking()\n",
      ")\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/teaching/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/hear21passt/models/passt.py:304: UserWarning: Input image size (128*1000) doesn't match model (128*998).\n",
      "  warnings.warn(f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\")\n"
     ]
    }
   ],
   "source": [
    "from hear21passt.base import get_basic_model,get_model_passt\n",
    "import torch\n",
    "# get the PaSST model wrapper, includes Melspectrogram and the default pre-trained transformer\n",
    "model = get_basic_model(mode=\"logits\")\n",
    "print(model.mel) # Extracts mel spectrogram from raw waveforms.\n",
    "print(model.net) # the transformer network.\n",
    "\n",
    "# example inference\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "with torch.no_grad():\n",
    "    # audio_wave has the shape of [batch, seconds*32000] sampling rate is 32k\n",
    "    # example audio_wave of batch=3 and 10 seconds\n",
    "    audio = torch.ones((3, 32000 * 10))*0.5\n",
    "    audio_wave = audio.cuda()\n",
    "    logits=model(audio_wave) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a7c5293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (163) must match the size of tensor b (99) at non-singleton dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     40\u001b[0m audio_batch \u001b[38;5;241m=\u001b[39m audio_batch\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 43\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLogits shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/hear21passt/wrapper.py:38\u001b[0m, in \u001b[0;36mPasstBasicWrapper.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m specs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel(x)\n\u001b[1;32m     37\u001b[0m specs \u001b[38;5;241m=\u001b[39m specs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m x, features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspecs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     40\u001b[0m     embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/hear21passt/models/passt.py:547\u001b[0m, in \u001b[0;36mPaSST.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    544\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m first_RUN\n\u001b[1;32m    545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_RUN: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m\"\u001b[39m, x\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m--> 547\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dist \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     features \u001b[38;5;241m=\u001b[39m (x[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m x[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/torch-gpu/lib/python3.10/site-packages/hear21passt/models/passt.py:494\u001b[0m, in \u001b[0;36mPaSST.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    492\u001b[0m     time_new_pos_embed \u001b[38;5;241m=\u001b[39m time_new_pos_embed[:, :, :, :x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]]\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m first_RUN: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m CUT time_new_pos_embed.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, time_new_pos_embed\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m--> 494\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtime_new_pos_embed\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_RUN: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m self.freq_new_pos_embed.shape\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_new_pos_embed\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    496\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreq_new_pos_embed\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (163) must match the size of tensor b (99) at non-singleton dimension 3"
     ]
    }
   ],
   "source": [
    "from hear21passt.base import get_basic_model\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Load your wav files  replace with your actual paths\n",
    "wav_paths = ['/home/teaching/DANN-pytorch/239946__rainbowdasheve__ghost.wav', '/home/teaching/DANN-pytorch/276470__unkleceeg__birds-chirping-03-short.wav', '/home/teaching/DANN-pytorch/182501__swiftoid__birds-chirping-03.wav']\n",
    "\n",
    "# Target sample rate for PaSST model\n",
    "target_sr = 32000\n",
    "\n",
    "def load_audio(path, target_sr=32000):\n",
    "    waveform, sr = torchaudio.load(path)  # [channels, samples]\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)\n",
    "    # Convert stereo to mono if needed\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    return waveform.squeeze(0)  # [samples]\n",
    "\n",
    "# Load and stack audio files into a batch tensor [batch, samples]\n",
    "audio_list = [load_audio(p, target_sr) for p in wav_paths]\n",
    "\n",
    "# Make all audio tensors the same length (pad shorter audios)\n",
    "max_len = max(a.shape[0] for a in audio_list)\n",
    "audio_padded = []\n",
    "for a in audio_list:\n",
    "    if a.shape[0] < max_len:\n",
    "        padding = torch.zeros(max_len - a.shape[0])\n",
    "        a = torch.cat([a, padding])\n",
    "    audio_padded.append(a)\n",
    "\n",
    "audio_batch = torch.stack(audio_padded)  # shape [batch_size, max_len]\n",
    "\n",
    "# Get model\n",
    "model = get_basic_model(mode=\"logits\")\n",
    "model.eval()\n",
    "model = model.cuda()\n",
    "\n",
    "# Move input to GPU\n",
    "audio_batch = audio_batch.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(audio_batch)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675a9859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio batch shape: torch.Size([3, 320000])\n",
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "Logits shape: torch.Size([3, 527])\n"
     ]
    }
   ],
   "source": [
    "from hear21passt.base import get_basic_model\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Your wav file paths\n",
    "wav_paths = [\n",
    "    '/home/teaching/DANN-pytorch/239946__rainbowdasheve__ghost.wav',\n",
    "    '/home/teaching/DANN-pytorch/276470__unkleceeg__birds-chirping-03-short.wav',\n",
    "    '/home/teaching/DANN-pytorch/182501__swiftoid__birds-chirping-03.wav'\n",
    "]\n",
    "\n",
    "target_sr = 32000\n",
    "target_length = target_sr * 10  # 10 seconds * 32000 samples/sec = 320,000 samples\n",
    "\n",
    "def load_audio(path, target_sr=32000):\n",
    "    waveform, sr = torchaudio.load(path)  # shape: [channels, samples]\n",
    "    if sr != target_sr:\n",
    "        waveform = torchaudio.transforms.Resample(sr, target_sr)(waveform)\n",
    "    # Convert to mono if stereo\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "    waveform = waveform.squeeze(0)  # shape: [samples]\n",
    "    \n",
    "    # Truncate or pad to target_length\n",
    "    if waveform.shape[0] > target_length:\n",
    "        waveform = waveform[:target_length]\n",
    "    elif waveform.shape[0] < target_length:\n",
    "        padding = torch.zeros(target_length - waveform.shape[0])\n",
    "        waveform = torch.cat([waveform, padding])\n",
    "    return waveform\n",
    "\n",
    "# Load all audios and stack into batch tensor of shape [batch, samples]\n",
    "audio_list = [load_audio(p, target_sr) for p in wav_paths]\n",
    "audio_batch = torch.stack(audio_list)\n",
    "\n",
    "print(f\"Audio batch shape: {audio_batch.shape}\")  # Should be [batch_size, 320000]\n",
    "\n",
    "# Load PaSST model\n",
    "model = get_basic_model(mode=\"embeddings\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Move batch to device\n",
    "audio_batch = audio_batch.to(device)\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    logits = model(audio_batch)\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef354ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2616, -4.3777, -4.6781,  ..., -5.1485, -5.2438, -7.6786],\n",
      "        [-3.7312, -7.0227, -6.4901,  ..., -6.7203, -8.4598, -6.6869],\n",
      "        [-2.3853, -7.2230, -7.1111,  ..., -7.8349, -9.6991, -7.2940]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1930457c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected Tensor as element 0 in argument 0, but got numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Load all audios and stack into batch tensor of shape [batch, samples]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m audio_list \u001b[38;5;241m=\u001b[39m [load_audio(p, target_sr) \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m wav_paths]\n\u001b[0;32m---> 30\u001b[0m audio_batch \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAudio batch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maudio_batch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Should be [batch_size, 320000]\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Load PaSST model in 'embed_only' mode\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected Tensor as element 0 in argument 0, but got numpy.ndarray"
     ]
    }
   ],
   "source": [
    "from hear21passt.base import get_basic_model\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# Your wav file paths\n",
    "wav_paths = [\n",
    "    '/home/teaching/DANN-pytorch/239946__rainbowdasheve__ghost.wav',\n",
    "    '/home/teaching/DANN-pytorch/276470__unkleceeg__birds-chirping-03-short.wav',\n",
    "    '/home/teaching/DANN-pytorch/182501__swiftoid__birds-chirping-03.wav'\n",
    "]\n",
    "\n",
    "target_sr = 32000\n",
    "target_length = target_sr * 10  # 10 seconds * 32000 samples/sec = 320,000 samples\n",
    "\n",
    "def load_audio(path, target_sr=32000):\n",
    "    # Load audio using librosa\n",
    "    waveform, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "    \n",
    "    # Truncate or pad to target_length\n",
    "    if len(waveform) > target_length:\n",
    "        waveform = waveform[:target_length]\n",
    "    elif len(waveform) < target_length:\n",
    "        padding = torch.zeros(target_length - len(waveform))\n",
    "        waveform = torch.cat([torch.tensor(waveform), padding])\n",
    "    \n",
    "    return waveform\n",
    "\n",
    "# Load all audios and stack into batch tensor of shape [batch, samples]\n",
    "audio_list = [load_audio(p, target_sr) for p in wav_paths]\n",
    "audio_batch = torch.stack(audio_list)\n",
    "\n",
    "print(f\"Audio batch shape: {audio_batch.shape}\")  # Should be [batch_size, 320000]\n",
    "\n",
    "# Load PaSST model in 'embed_only' mode\n",
    "model = get_basic_model(mode=\"embed_only\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Move batch to device\n",
    "audio_batch = audio_batch.to(device)\n",
    "\n",
    "# Forward pass to get embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(audio_batch)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92f0c79b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio batch shape: torch.Size([3, 320000])\n",
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "Embeddings shape: torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from hear21passt.base import get_basic_model\n",
    "\n",
    "# Your .wav file paths\n",
    "wav_paths = [\n",
    "    '/home/teaching/DANN-pytorch/239946__rainbowdasheve__ghost.wav',\n",
    "    '/home/teaching/DANN-pytorch/276470__unkleceeg__birds-chirping-03-short.wav',\n",
    "    '/home/teaching/DANN-pytorch/182501__swiftoid__birds-chirping-03.wav'\n",
    "]\n",
    "\n",
    "# Parameters\n",
    "target_sr = 32000\n",
    "target_length = target_sr * 10  # 10 seconds * 32000 samples/sec\n",
    "\n",
    "def load_audio(path, target_sr=32000):\n",
    "    # Load audio with librosa (mono=True ensures single channel)\n",
    "    waveform, sr = librosa.load(path, sr=target_sr, mono=True)\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "    # Pad or truncate to fixed length\n",
    "    if waveform.shape[0] < target_length:\n",
    "        padding = torch.zeros(target_length - waveform.shape[0], dtype=torch.float32)\n",
    "        waveform = torch.cat([waveform, padding])\n",
    "    else:\n",
    "        waveform = waveform[:target_length]\n",
    "\n",
    "    return waveform\n",
    "\n",
    "# Load and process all audio files\n",
    "audio_list = [load_audio(p, target_sr) for p in wav_paths]\n",
    "audio_batch = torch.stack(audio_list)  # Shape: [batch_size, 320000]\n",
    "print(f\"Audio batch shape: {audio_batch.shape}\")\n",
    "\n",
    "# Load PaSST model in 'embed_only' mode\n",
    "model = get_basic_model(mode=\"embed_only\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Move audio to device\n",
    "audio_batch = audio_batch.to(device)\n",
    "\n",
    "# Forward pass to extract embeddings\n",
    "with torch.no_grad():\n",
    "    embeddings = model(audio_batch)\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")  # Expected: [batch_size, embedding_dim]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61a65ac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([3, 768])\n",
      "Embeddings:\n",
      " tensor([[ 0.4242,  0.8466, -0.5205,  ...,  0.2797,  0.8814, -1.3569],\n",
      "        [-1.3094,  0.8903,  0.0708,  ...,  0.8115, -0.4800, -0.5528],\n",
      "        [-1.3966,  0.6613, -0.3923,  ..., -0.5098,  0.1874, -0.1976]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings shape: {embeddings.shape}\")  # Expected: [batch_size, embedding_dim]\n",
    "print(\"Embeddings:\\n\", embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "270c2eb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: FMAX is None setting to 15000 \n",
      "\n",
      "\n",
      " Loading PASST TRAINED ON AUDISET \n",
      "\n",
      "\n",
      "PaSST(\n",
      "  (patch_embed): PatchEmbed(\n",
      "    (proj): Conv2d(1, 768, kernel_size=(16, 16), stride=(10, 10))\n",
      "    (norm): Identity()\n",
      "  )\n",
      "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "  (blocks): Sequential(\n",
      "    (0): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (1): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (2): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (3): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (4): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (5): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (6): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (7): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (8): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (9): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (10): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (11): Block(\n",
      "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (attn): Attention(\n",
      "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
      "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
      "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (drop_path): Identity()\n",
      "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "      (mlp): Mlp(\n",
      "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "        (act): GELU(approximate='none')\n",
      "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "        (drop): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
      "  (pre_logits): Identity()\n",
      "  (head): Sequential(\n",
      "    (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=768, out_features=527, bias=True)\n",
      "  )\n",
      "  (head_dist): Linear(in_features=768, out_features=527, bias=True)\n",
      ")\n",
      "Embedding shape: torch.Size([1, 768])\n",
      "Embedding vector:\n",
      "tensor([ 4.2420e-01,  8.4659e-01, -5.2048e-01,  7.6492e-01,  9.1242e-01,\n",
      "        -2.7631e-01, -6.6285e-02,  8.7862e-01, -7.7146e-01,  1.5370e+00,\n",
      "         9.3644e-01,  5.0816e-01,  1.0239e+00,  4.8391e-01,  2.3251e-01,\n",
      "         1.8595e+00,  3.4335e-03, -1.3167e+00,  1.0204e+00, -1.4823e-01,\n",
      "         2.9429e-01,  1.0178e+00,  5.4007e-01,  1.1090e+00,  1.1697e-02,\n",
      "        -5.2966e-01, -2.1295e-01,  9.9255e-01, -4.7931e-01, -3.3757e-01,\n",
      "        -7.2715e-02,  2.0662e-01, -5.5332e-01,  2.0980e+00,  7.9912e-01,\n",
      "        -4.2069e-01, -1.8577e+00, -6.1272e-01, -5.1383e-01,  4.3833e-01,\n",
      "         5.8067e-01,  9.9178e-01, -5.5711e-01, -4.3412e-01,  4.2106e-01,\n",
      "         2.3469e+00,  2.8104e-02,  4.2875e-01,  8.0761e-01,  2.0481e-01,\n",
      "         1.0778e+00,  6.2422e-01,  2.7659e+00, -4.3241e-01, -1.0865e+00,\n",
      "         1.5589e+00, -1.1069e+00, -9.3780e-01, -6.6037e-01, -1.2097e+00,\n",
      "        -7.3824e-01,  1.9774e+00, -4.6350e-01, -2.4741e+00,  1.6988e+00,\n",
      "         1.5441e+00, -3.7804e-01, -8.4214e-01,  3.6020e-01, -7.1560e-01,\n",
      "        -1.0588e+00,  3.5801e-01, -1.7813e+00, -4.0069e-01, -2.5586e-01,\n",
      "        -5.9017e-02,  9.7018e-01,  1.0575e-01, -9.8256e-01, -1.3216e+00,\n",
      "         8.8240e-01,  1.0951e+00,  7.6858e-01,  3.5605e-02,  1.2140e+00,\n",
      "        -2.2099e-02,  2.9042e-01, -1.9386e-02, -1.4346e+00,  4.8002e-01,\n",
      "        -3.6500e-02,  1.6344e+00, -1.1530e+00,  1.1538e+00,  3.0915e+00,\n",
      "        -5.9493e-01, -6.3228e-01, -6.1319e-01, -3.1152e-01, -6.9710e-01,\n",
      "        -3.6750e-01, -1.1233e+00, -8.9125e-01, -6.5377e-01,  2.6774e-01,\n",
      "         9.6705e-01, -2.0032e+00, -1.7700e-01, -1.0231e+00,  1.0774e+00,\n",
      "        -2.5072e+00,  3.1163e+00,  1.4778e+00,  1.0718e+00, -6.8395e-01,\n",
      "        -5.2372e-02,  1.3342e-01,  2.4613e+00, -1.2549e+00,  1.4896e+00,\n",
      "         1.7759e+00,  1.9961e+00, -5.9058e-01,  1.6600e+00,  1.5022e+00,\n",
      "        -5.8364e-01,  6.8775e-01,  1.1223e+00, -1.8710e+00, -2.4446e-01,\n",
      "        -7.5143e-01,  1.3283e+00, -9.1006e-01, -2.3540e+00,  7.1166e-01,\n",
      "        -2.2852e+00,  6.7978e-01,  7.4428e-01, -1.3713e+00, -5.9354e-01,\n",
      "        -6.0188e-01,  4.8094e-03, -3.6761e-02, -1.1143e+00, -1.4641e+00,\n",
      "        -5.0794e-01,  2.2346e+00, -2.0352e-01,  9.0775e-01, -3.9005e-01,\n",
      "         1.4068e+00,  1.6165e+00, -7.7465e-01,  2.5870e-01,  7.5142e-01,\n",
      "         1.5668e+00,  1.5581e+00, -2.2326e+00,  1.3995e+00,  8.2120e-01,\n",
      "        -5.0430e-01, -2.4944e-01,  2.2047e+00,  2.6380e-03, -3.1404e-01,\n",
      "        -8.5674e-02,  1.7661e+00,  2.1138e-01,  3.9037e-01, -1.7476e+00,\n",
      "         2.8560e+00,  6.7590e-01, -5.7877e-01,  1.6565e-01, -5.4611e-01,\n",
      "         7.1001e-01, -1.4557e-01, -9.6166e-01,  4.6627e-01, -2.9965e-01,\n",
      "        -3.8376e-01, -2.7353e-01,  8.1193e-01,  2.5157e+00, -1.5117e+00,\n",
      "         9.2569e-01,  1.5022e+00,  1.8795e-01, -1.3627e-01, -1.5898e+00,\n",
      "         6.4675e-01, -1.0510e-01, -9.3457e-01, -1.0202e+00, -1.4301e-01,\n",
      "        -8.8776e-01,  4.9613e-01,  2.0134e-01,  1.2550e+00, -1.5273e+00,\n",
      "         9.8366e-01, -6.0050e-01,  1.2400e+00,  2.9275e-01,  1.6210e-02,\n",
      "         2.1641e-01,  4.8486e-01, -2.9144e-01,  3.2354e-01,  4.2261e-01,\n",
      "         3.7203e-01,  1.3896e+00,  1.2013e+00,  1.9076e+00,  2.3386e-01,\n",
      "        -4.3772e-01,  3.0724e-01,  7.8164e-01,  2.0435e+00, -1.0712e+00,\n",
      "         1.1151e+00, -9.0129e-01, -1.6868e-01, -1.2957e+00, -1.0851e-01,\n",
      "        -9.2801e-01,  2.2488e+00, -9.5563e-01,  4.4119e-01, -1.1635e+00,\n",
      "        -4.4978e-01,  5.3616e-01,  2.1249e+00,  1.0420e+00, -1.5380e+00,\n",
      "        -5.3902e-01,  1.9406e+00,  3.3510e-01,  1.3868e+00, -3.8106e-02,\n",
      "        -9.4038e-01, -2.8157e-01,  4.7672e-01,  3.5017e-01, -7.0356e-01,\n",
      "        -1.0555e-01, -1.7417e+00, -2.3641e-01, -1.5593e+00, -1.9811e-01,\n",
      "         8.2569e-03, -1.9401e+00,  2.2822e+00,  8.5190e-01, -9.2954e-01,\n",
      "         1.0493e+00,  1.8163e+00, -4.3880e-01, -2.5583e+00,  2.9890e-01,\n",
      "         6.2652e-01,  5.2568e-01, -5.7537e-01,  5.3408e-01,  3.6171e-01,\n",
      "         7.2267e-01,  2.9519e-01, -3.7060e-02, -5.1771e-01,  1.7461e-01,\n",
      "        -1.3106e+00, -1.0002e-01, -1.2564e-01, -2.1604e-01, -2.6978e-01,\n",
      "         3.3110e-01, -3.3964e-01, -3.3734e-01,  6.7880e-01,  1.0139e+00,\n",
      "        -5.6627e-01, -5.8821e-01, -9.6506e-01,  9.9924e-01, -8.0668e-01,\n",
      "        -9.5913e-01,  3.6751e-01, -2.1981e-01, -9.6776e-01,  8.8377e-01,\n",
      "        -4.7042e-01,  1.9576e+00, -4.7929e-01, -6.9533e-01,  1.7924e-01,\n",
      "         1.9940e+00,  5.9207e-02, -6.8593e-01, -1.6137e+00, -4.7047e-01,\n",
      "        -4.2648e-01,  1.3235e+00, -5.1174e-01, -7.1974e-01,  4.7072e-01,\n",
      "         3.4421e-01, -4.2373e-04, -8.7832e-02,  1.3257e-01,  6.3918e-01,\n",
      "        -4.8021e-01, -1.5333e+00, -7.2866e-01,  6.9522e-01, -5.8129e-01,\n",
      "        -9.3098e-01,  1.1916e+00, -2.6624e-01,  1.5667e+00, -1.2525e+00,\n",
      "         1.2201e+00,  2.3591e+00,  1.5389e-01,  3.8146e-01, -9.9814e-01,\n",
      "         5.0346e-01,  4.7448e-01,  1.1342e+00, -1.2940e+00,  9.3457e-01,\n",
      "        -3.4153e-01, -1.1823e+00,  6.3855e-01, -3.1076e-01, -1.6351e+00,\n",
      "         1.6852e+00, -8.8044e-01,  9.1126e-02, -1.5759e+00,  1.4466e+00,\n",
      "         4.4022e-01,  5.5400e-01, -1.1763e+00, -3.4638e-01, -1.4844e+00,\n",
      "         1.3478e+00, -3.7808e-01, -4.9182e-01, -8.7615e-01, -5.3681e-01,\n",
      "         2.4742e+00,  1.8097e-01, -2.2328e+00,  3.3559e+00,  2.2560e+00,\n",
      "         9.4177e-01, -2.4796e+00, -2.1129e+00,  1.2535e+00, -8.5987e-01,\n",
      "        -1.3549e-01,  2.0285e-01, -1.5208e+00,  1.3241e+00,  1.1184e-01,\n",
      "        -7.8086e-01,  4.8228e-01,  9.3934e-01,  1.0368e+00, -6.2040e+00,\n",
      "         7.7439e-01,  1.5914e-01,  1.5752e-01,  5.8544e-01,  8.8490e-01,\n",
      "        -1.2696e+00,  5.2181e-02,  7.0833e-01,  1.0076e+00, -7.8456e-01,\n",
      "        -4.4818e-01, -1.2868e-01,  5.3322e-01, -5.1332e-01,  1.2232e+00,\n",
      "        -7.3517e-01, -1.4753e-01, -1.8733e+00,  1.0395e+00, -9.1583e-01,\n",
      "         1.4878e-02, -3.3201e-01,  4.9207e-02,  1.8664e-03,  9.7422e-01,\n",
      "         7.0772e-01, -1.5848e+00, -9.6570e-01, -3.0793e-01, -1.0849e+00,\n",
      "        -2.6438e-01,  5.6302e+00,  1.9896e+00,  5.7489e-01,  6.9446e-01,\n",
      "        -3.5845e-01, -7.9192e-02,  4.3677e-01, -1.3141e-02, -1.3668e+00,\n",
      "        -3.2290e+00,  2.2734e+00, -1.1091e+00,  9.7499e-01, -1.6121e+00,\n",
      "        -8.3525e-01, -2.0160e+00, -3.0874e+00, -1.9767e+00,  2.3971e+00,\n",
      "         2.1954e-01,  2.5795e-01,  2.7663e-02, -4.5734e-01, -1.3529e+00,\n",
      "         1.8103e+00,  1.5735e+00, -1.4015e-01,  1.9873e+00,  1.1506e+00,\n",
      "        -9.5144e-01, -9.1131e-01,  1.1122e+00,  6.9222e-01, -1.2685e-01,\n",
      "         2.0386e-01, -2.7483e-01,  1.4970e-01, -2.3541e-01, -1.6699e+00,\n",
      "         6.2726e-01,  6.0243e-01, -1.4054e+00,  3.7403e-01,  1.0427e+00,\n",
      "         4.8396e-01, -7.4431e-01,  1.4114e+00, -1.4410e+00, -6.5909e-01,\n",
      "         1.7761e+00,  2.1715e-01, -7.4147e-01,  1.0574e+00, -8.7192e-01,\n",
      "        -2.9592e-01, -2.0484e+00,  4.6750e-01, -2.0549e+00,  2.4494e-01,\n",
      "         2.0304e-01, -1.1168e+00,  2.3660e-01,  8.8896e-01, -8.7386e-01,\n",
      "         1.9873e-01, -2.1034e-01, -1.3267e-01, -1.2786e+00,  9.5323e-01,\n",
      "        -1.3011e+00, -1.0690e+00, -9.8370e-01,  1.4437e+00,  8.2911e-01,\n",
      "         1.6077e-01,  7.4837e-01,  2.9522e-01,  1.8950e-01, -8.3914e-01,\n",
      "        -5.9539e-01, -1.2835e+00, -1.7769e+00,  1.1837e+00,  1.4598e+00,\n",
      "        -5.4643e-01, -2.6264e+00, -7.3388e-01,  1.8919e+00, -6.2845e-01,\n",
      "        -9.2305e-01,  4.2882e-03, -6.3998e-01,  1.3020e+00,  1.3526e+00,\n",
      "         1.9575e+00, -5.1967e-01,  1.2351e+00,  1.0271e+00, -5.6773e-02,\n",
      "         7.5987e-01, -8.8062e-01, -1.3087e+00,  8.9596e-01,  3.9954e-01,\n",
      "        -1.6114e+00,  6.8390e-01, -8.7751e-02,  7.0982e-02, -7.0892e-01,\n",
      "         4.7060e-01,  1.3441e+00, -1.0634e+00, -1.0854e+00,  2.2131e-01,\n",
      "        -7.0729e-01,  6.5714e-01,  7.1224e-01,  8.1471e-01,  1.6982e+00,\n",
      "        -4.6277e-01, -2.2299e-01,  5.9742e-02, -1.2254e+00, -6.5700e-01,\n",
      "        -3.2683e-01, -5.3592e-01, -4.3761e-01, -4.9878e-01,  1.8546e-01,\n",
      "         6.5452e-02,  3.0074e-01,  1.2824e+00, -1.1149e+00,  6.7747e-01,\n",
      "        -2.4371e+00,  6.0269e-01, -1.6721e+00,  1.7077e-01,  1.8043e-02,\n",
      "        -1.0698e+00, -1.1465e+00, -1.0201e+00, -1.9804e+00,  1.0676e+00,\n",
      "        -1.9907e-01, -3.3415e-01, -4.4214e-01,  2.1832e+00, -1.6235e+00,\n",
      "        -8.3958e-01,  1.2502e+00,  4.7772e-02, -2.5746e-01, -1.7961e+00,\n",
      "        -7.5701e-01, -4.3736e-01,  2.9111e-01, -9.7733e-01, -1.3815e+00,\n",
      "         1.8219e-01,  9.6851e-01, -4.6914e-01, -9.0240e-01,  2.0960e-01,\n",
      "        -2.2428e-01, -2.2438e-01, -1.3194e+00,  1.2122e+00, -1.2514e-01,\n",
      "        -2.9460e-01,  1.3948e+00,  2.8237e+00, -7.8761e-01,  1.6426e+00,\n",
      "        -1.4372e+00, -1.3021e+00,  1.6569e+00, -2.0025e+00,  5.3277e-01,\n",
      "        -2.2660e-01, -9.1719e-01, -1.9484e+00, -3.4152e-01, -1.7412e+00,\n",
      "        -3.4495e-01, -1.4422e-01, -7.6479e-01,  3.4148e-01, -3.4990e-01,\n",
      "         6.9757e-01,  2.0315e-01,  2.4397e+00,  1.7110e-01,  1.0682e+00,\n",
      "        -1.0768e-02, -5.2067e-01,  8.1219e-01,  1.9353e+00,  1.0860e+00,\n",
      "        -5.5671e-02, -4.4734e-01, -1.1261e+00,  9.0208e-01, -2.8011e+00,\n",
      "         2.3237e-01, -1.3346e+00,  6.7268e-01,  7.8909e-01,  6.1178e-02,\n",
      "         1.3769e-01, -8.9179e-02, -1.5745e+00, -3.4343e-01, -1.3721e+00,\n",
      "        -2.1191e-01,  2.3310e-01, -1.3101e+00, -7.0539e-01,  1.6285e-01,\n",
      "        -5.6978e-01,  9.3384e-01, -1.1503e-01, -5.3396e-01, -1.6867e+00,\n",
      "         7.2282e-01,  1.2760e+00,  7.8629e-01,  1.7290e+00, -1.0095e+00,\n",
      "         1.7250e-01,  5.6113e-01, -2.0382e+00,  9.8111e-01,  3.8620e-02,\n",
      "         3.1912e-01,  1.2287e+00,  3.1511e+00, -1.8875e+00,  1.4192e-01,\n",
      "         1.6203e+00, -6.4835e-01, -1.7123e+00, -6.2794e-01,  2.7460e-01,\n",
      "         1.4639e+00, -4.5011e-02, -3.5522e-01, -9.3731e-01, -4.5762e-01,\n",
      "        -7.5300e-01, -9.3731e-01, -8.6460e-01, -1.1811e+00,  1.2193e+00,\n",
      "         1.0666e-01, -2.1480e+00, -7.1955e-01,  9.8860e-01, -2.6910e-01,\n",
      "         1.8288e+00, -9.1371e-01,  1.9505e-01,  2.1276e-01,  1.8188e-01,\n",
      "         1.0530e+00, -1.0035e-01,  2.8658e-01,  7.4472e-01, -6.2356e-02,\n",
      "        -5.3789e-01,  4.3508e-01,  3.8999e-01,  1.1673e+00,  2.1189e+00,\n",
      "         1.1275e-01,  1.3667e+00,  3.0939e-01,  9.3553e-01, -1.1539e+00,\n",
      "         5.8051e-01, -6.3525e-01,  1.0402e+00,  1.5438e+00, -1.4575e+00,\n",
      "        -1.4551e+00,  2.1935e+00,  1.4266e+00,  1.2821e-01, -2.0114e-01,\n",
      "         2.5303e-02, -4.1752e-01, -2.0931e+00, -1.0024e+00,  5.5586e-01,\n",
      "        -6.8201e-01,  1.8866e+00,  1.8421e+00, -2.5226e-01,  6.2276e-01,\n",
      "         1.0960e+00,  2.1006e+00, -2.7172e+00,  1.6728e-01,  3.6136e-01,\n",
      "        -9.3389e-01, -6.8588e-01, -7.3668e-01,  1.1096e+00,  7.1442e-02,\n",
      "        -2.9500e-01,  1.6572e+00, -1.3166e-01, -5.6145e-01, -1.6325e+00,\n",
      "         1.1063e-01, -5.2522e-01, -1.0089e-01,  1.3581e-02, -1.0815e+00,\n",
      "         1.4097e+00, -1.0427e+00, -1.9598e-01, -6.9929e-01, -1.1439e+00,\n",
      "         8.3410e-01,  7.9867e-01, -1.2729e+00,  3.2797e-01,  4.9159e-01,\n",
      "        -1.2425e+00, -5.6820e-01, -6.6616e-01, -9.5787e-01,  1.3321e+00,\n",
      "        -6.3167e-01, -1.9748e-01, -1.3771e-01, -3.6187e-01,  1.4057e+00,\n",
      "         8.2005e-01, -1.1946e+00, -2.0024e+00,  3.0911e-01,  1.0210e+00,\n",
      "         4.8837e-01,  4.3156e-01, -8.6407e-01,  1.6093e-01, -2.6510e-02,\n",
      "        -7.2800e-01, -7.6964e-02, -1.1445e+00,  1.3790e+00,  1.7316e+00,\n",
      "         7.1298e-02,  6.4669e-01,  9.5734e-01, -3.0448e-01,  1.6308e+00,\n",
      "        -5.7255e-01, -7.3343e-01,  5.1102e-01, -8.2974e-01, -1.1740e+00,\n",
      "         2.7967e-01,  8.8142e-01, -1.3569e+00], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from hear21passt.base import get_basic_model\n",
    "\n",
    "# Path to a single audio file\n",
    "wav_path = '/home/teaching/DANN-pytorch/239946__rainbowdasheve__ghost.wav'\n",
    "\n",
    "# Parameters\n",
    "target_sr = 32000\n",
    "target_length = target_sr * 10  # 10 seconds\n",
    "\n",
    "# Load and preprocess the audio\n",
    "waveform, sr = librosa.load(wav_path, sr=target_sr, mono=True)\n",
    "waveform = torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "# Pad or truncate\n",
    "if waveform.shape[0] < target_length:\n",
    "    padding = torch.zeros(target_length - waveform.shape[0], dtype=torch.float32)\n",
    "    waveform = torch.cat([waveform, padding])\n",
    "else:\n",
    "    waveform = waveform[:target_length]\n",
    "\n",
    "# Add batch dimension: [1, samples]\n",
    "audio_batch = waveform.unsqueeze(0)\n",
    "\n",
    "# Load PaSST model in 'embed_only' mode\n",
    "model = get_basic_model(mode=\"embed_only\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "audio_batch = audio_batch.to(device)\n",
    "\n",
    "# Extract embedding\n",
    "with torch.no_grad():\n",
    "    embedding = model(audio_batch)  # shape: [1, 768]\n",
    "\n",
    "print(f\"Embedding shape: {embedding.shape}\")\n",
    "print(f\"Embedding vector:\\n{embedding.squeeze(0)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5fc1b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding shape: torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding shape: {embedding.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
